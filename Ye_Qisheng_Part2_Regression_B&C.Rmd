---
title: "Fall 2022: Final Project"
author: "Qisheng Ye"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

This RMarkdown shows how to read in the final project data. It also shows how to calculate the derived input features and how to derive the categorical output from the continuous output. It also demonstrates how to fit a simple model (with `lm()`), save that model, and load it back into the workspace. You may find these actions helpful as you work through the project.  

**You must download the data from Canvas and save the data in the same directory as this RMarkdown file.**  

## Load packages

This example uses the `tidyverse` suite of packages.  

```{r, load_tidyverse}
library(tidyverse)
```

## Read data

Please download the final project data from Canvas. If this Rmarkdown file is located in the same directory as the downloaded CSV file, it will be able to load in the data for you. It is **highly** recommended that you use an RStudio RProject to more easily manage the working directory and file paths of the code and objects associated with the final project.  

The code chunk below reads in the final project data.  

```{r, read_final_data}
df <- readr::read_csv("fall2022_finalproject.csv", col_names = TRUE)
dfnew <- readr::read_csv("fall2022_holdout_inputs.csv", col_names = TRUE)
```

The `readr::read_csv()` function displays the data types and column names associated with the data. However, a glimpse is shown below that reveals the number of rows and also shows some of the representative values for the columns.  

```{r, show_data_glimpse}
df %>% glimpse()
```

The data have continuous inputs and a categorical input. The continuous inputs consist of two groups of variables, the "x-variables", `x1` through `x4`, and the "v-variables", `v1` through `v5`. The categorical input is `m`. The response is continuous and is named `output`.  


## Simple model

Let's fit a simple linear model for `output`. We will use a linear relationship with a single input, `x1`, for demonstration purposes. The model is fit using the formula interface below and assigned to the `mod01` object.  

```{r, fit_mod01}
#mod01 <- lm( output ~ x1, data = df )
```


The model fitting results are summarized below with a call to the `summary()` function.  

```{r, show_mod01_summary}
#mod01 %>% summary()
```

### Save model

Let’s go ahead and save `mod01`. There are multiple approaches for saving objects including `.Rda` and `.rds`. I prefer to use the `.rds` object because it’s more streamlined and makes it easier to save and reload a single object, which in our case is a model object. We can use the base `R` `saveRDS()` function or the `tidyverse` equivalent `write_rds()` function from the `readr` package. I prefer to use the `tidyverse` version.

The code chunk below pipes the `mod01` object into `readr::write_rds()`. It saves the object to a file in the local working directory for simplicity. Notice that the `.rds` extension is included after the desired file name.  

```{r, save_mod01}
#mod01 %>% readr::write_rds("my_simple_example_model.rds")
```

If you ran the above code chunk, check your working directory with the Files tab. You should see the `my_simple_example_model.rds` in your current working directory.

### Reload model

Let’s now load in that model, but assign it to a different variable name. We can read in an `.rds` file with the `readr::read_rds()` function. The object is loaded in and assigned to the `re_load_mod01` object in the code chunk below.  

```{r, reload_mod01}
#re_load_mod01 <- readr::read_rds("my_simple_example_model.rds")
lm_mod_yours_2 <- readr::read_rds("lm_mod_yours_2.rds")
lm_mod_yours_3 <- readr::read_rds("lm_mod_yours_3.rds")
```


We can now work with the `re_load_mod01` object just like the original model we fit, `mod01`. So we can use `summary()` and any other function on the model object, like `predict()`. To confirm let’s print out the summary below. If you compare the summary results to that printed previously you will see that the two are identical.  

```{r, show_reload_summary}
#re_load_mod01 %>% summary()
```

And to confirm let's check that the model objects are the same with the `all.equal()` function.  

```{r, check_mod01_equal}
#all.equal( mod01, re_load_mod01 )
```

## Derived quantities

One of the goals of the final project is for you to assess if Subject Matter Expert (SME) recommended features help improve model performance relative to using the as-collected "x-" and "v-" input variables. The input derived *features* are calculated for you in the code chunk below using the `mutate()` function and a glimpse of the resulting data set is displayed to the screen. This is shown to demonstrate how to calculate these derived features from the provided input variables.  

```{r, show_derived_features}
df <- df %>% 
  mutate(x5 = 1 - (x1 + x2 + x3 + x4),
         w = x2 / (x3 + x4),
         z = (x1 + x2) / (x5 + x4),
         t = v1 * v2) %>% 
  glimpse()

input <- df

dfnew <- dfnew %>% 
  mutate(x5 = 1 - (x1 + x2 + x3 + x4),
         w = x2 / (x3 + x4),
         z = (x1 + x2) / (x5 + x4),
         t = v1 * v2) %>% 
  glimpse()

input <- dfnew
```


You are required as part of the project to explore the data. Your exploration will demonstrate that `output`, the continuous response, is between 0 and 1. Because of this, it is **highly recommended** that you transform the continuous response before training regression models. You should use the logit transformation to convert the lower and upper bounded `output` variable to an unbounded variable. The regression models should be trained to predict the logit-transformed response. The code chunk below shows how to calculate the unbounded response, `y`, as the logit transformation of the `output` variable.  

```{r, show_logit_transform}
df <- df %>% 
  mutate(y = boot::logit(output)) %>% 
  glimpse()
```



Although the response is continuous and you will be working with regression models in this project, you will also train binary classification models. To do so, you must derive a binary response from the continuous response, `output`. You will train classification models to classify the event of interest, which corresponds to `output < 0.33`. The binary response, `outcome`, is calculated in the code chunk below with an `ifelse()` call. The two levels are `'event'` and `non_event'`. The `outcome` column is converted to a factor variable (categorical variable) with the first level assigned to `'event'`. You are required to use this setup for the binary variable that way everyone will work with a consistent binary output.  

```{r, show_binary_outcome}
df <- df %>% 
  mutate(outcome = ifelse(output < 0.33, 'event', 'non_event'),
         outcome = factor(outcome, levels = c("event", "non_event"))) %>% 
  glimpse()
```


###Part ii: Regression - iiB)Bayesian Linear models
```{r}
X_yours_3 <- model.matrix(lm_mod_yours_3)

info_yours_3<- list(
  yobs = df$y,
  design_matrix = X_yours_3,
  mu_beta = 0,
  tau_beta = 50,
  sigma_rate = 1
)

X_yours_2 <- model.matrix(lm_mod_yours_2)

info_yours_2<- list(
  yobs = df$y,
  design_matrix = X_yours_2,
  mu_beta = 0,
  tau_beta = 50,
  sigma_rate = 1
)

```

```{r}
lm_logpost <- function(unknowns, my_info)
{
  # specify the number of unknown beta parameters
  length_beta <- ncol(my_info$design_matrix)
  
  # extract the beta parameters from the `unknowns` vector
  beta_v <- unknowns[1:length_beta]
  
  # extract the unbounded noise parameter, varphi
  lik_varphi <- unknowns[length_beta + 1]
  
  # back-transform from varphi to sigma
  lik_sigma <- exp(lik_varphi)
  
  # extract design matrix
  X <- my_info$design_matrix
  
  # calculate the linear predictor
  mu <- as.vector( X %*% as.matrix(beta_v) )
  
  # evaluate the log-likelihood
  log_lik <- sum(dnorm(x = my_info$yobs,
                       mean = mu,
                       sd = lik_sigma,
                       log = TRUE))
  
  # evaluate the log-prior
  log_prior_beta <- sum(dnorm(x = beta_v,
                              mean = my_info$mu_beta,
                              sd = my_info$tau_beta,
                              log = TRUE))
  
  log_prior_sigma <- dexp(x = lik_sigma,
                          rate = my_info$sigma_rate,
                          log = TRUE)
  
  # add the mean trend prior and noise prior together
  log_prior <- log_prior_beta + log_prior_sigma
  
  # account for the transformation
  log_derive_adjust <- lik_varphi
  
  # sum together
  log_lik + log_prior + log_derive_adjust
}
```

```{r}
my_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian = TRUE,
               control = list(fnscale = -1, maxit = 1001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian)
  p <- length(mode)
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}
```

```{r}
viz_post_coefs <- function(post_means, post_sds, xnames)
{
  tibble::tibble(
    mu = post_means,
    sd = post_sds,
    x = xnames
  ) %>% 
    mutate(x = factor(x, levels = xnames)) %>% 
    ggplot(mapping = aes(x = x)) +
    geom_hline(yintercept = 0, color = 'grey', linetype = 'dashed') +
    geom_point(mapping = aes(y = mu)) +
    geom_linerange(mapping = aes(ymin = mu - 2 * sd,
                                 ymax = mu + 2 * sd,
                                 group = x)) +
    labs(x = 'feature', y = 'coefficient value') +
    coord_flip() +
    theme_bw()
}

```

```{r}
laplace_yours_3 <- my_laplace(rep(0, ncol(X_yours_3)+1), lm_logpost, info_yours_3)

laplace_yours_2 <- my_laplace(rep(0, ncol(X_yours_2)+1), lm_logpost, info_yours_2)
```

```{r}
viz_post_coefs(laplace_yours_3$mode[1:ncol(X_yours_3)],
               sqrt(diag(laplace_yours_3$var_matrix)[1:ncol(X_yours_3)]),
               colnames(X_yours_3))
```


```{r}
viz_post_coefs(laplace_yours_2$mode[1:ncol(X_yours_2)],
               sqrt(diag(laplace_yours_2$var_matrix)[1:ncol(X_yours_2)]),
               colnames(X_yours_2))
```

```{r}
exp(laplace_yours_3$log_evidence-laplace_yours_2$log_evidence)
```
Since the Bayes Factor result of lm_mod_yours_3 is bigger, the lm_mod_yours_3 is better.

Question: How does the lm() maximum likelihood estimate (MLE) on 𝜎 relate to the posterior uncertainty on 𝜎?
Comparing the MAP equation with MLE, we can find that the only difference is that MAP contains a priori in the formula, which means that the likelihood is weighted by the priori in MAP.

```{r}
min(df$x1)
max(df$x1)
min(df$x2)
```

```{r}
tidy_predict <- function(mod, xnew)
{
  pred_df <- predict(mod, xnew, interval = "confidence") %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    dplyr::select(pred = fit, ci_lwr = lwr, ci_upr = upr) %>% 
    bind_cols(predict(mod, xnew, interval = 'prediction') %>% 
                as.data.frame() %>% tibble::as_tibble() %>% 
                dplyr::select(pred_lwr = lwr, pred_upr = upr))
  
  xnew %>% bind_cols(pred_df)
}
```

```{r}
pred_lm_01 <- tidy_predict(lm_mod_yours_3, dfnew)
```


```{r}
pred_lm_01 %>% 
  ggplot(mapping = aes(x = x1)) +
  geom_ribbon(mapping = aes(ymin = pred_lwr, ymax = pred_upr),
              fill = 'blue') +
  geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr),
              fill = 'orange') +
  geom_line(mapping = aes(y = pred),
            color = 'black') +
  coord_cartesian(ylim = c(-7, 7)) +
  facet_wrap(~m, labeller = "label_both") +
  theme_bw()
```
```{r}
pred_lm_02 <- tidy_predict(lm_mod_yours_2, dfnew)
```


```{r}
pred_lm_02 %>% 
  ggplot(mapping = aes(x = x1)) +
  geom_ribbon(mapping = aes(ymin = pred_lwr, ymax = pred_upr),
              fill = 'blue') +
  geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr),
              fill = 'orange') +
  geom_line(mapping = aes(y = pred),
            color = 'black') +
  coord_cartesian(ylim = c(-7, 7)) +
  facet_wrap(~m, labeller = "label_both") +
  theme_bw()
```
Predictive trends are almost consistent between the 2selected linear models.